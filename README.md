# Softplus-Power Activation

Softplus-Power is a **biologically inspired activation function** designed as a smooth, flexible alternative to ReLU, SiLU/Swish, and Softplus.  
It is motivated by real neuronal contrast-response functions and can emulate different neuron classes via parameter presets.

---

## ğŸ“Œ Overview
 Most deep learning models rely on simple activation functions like **ReLU** or **SiLU**. While effective, these functions are far removed from the graded, saturating, and adaptive responses of real neurons in the brain.

 **Softplus-Power** is a  generalization of the classic **Softplus** activation:

 $$
 f(x) = \alpha \cdot \big( \ln(1 + e^{\beta(x - \theta)}) \big)^{p} - \delta
 $$

 where:
 - **Î±** = output gain  
 - **Î²** = slope (input scaling)  
 - **Î¸** = threshold shift  
 - **p** = curvature (exponent)  
 - **Î´** = output offset  

ğŸ‘‰ This parametrization allows Softplus-Power to smoothly interpolate between **ReLU-like**, **sigmoid-like**, and **biologically plausible saturating** responses.

---

## âœ¨ Features

- ğŸš€ **Cross-framework support**: PyTorch, Keras/TensorFlow, JAX, and NumPy reference implementations.  
- ğŸ”’ **Calibrated variants**: guarantees Î±, Î² > 0 and p â‰¥ 0.5 for stable training and well-behaved gradients.  
- ğŸ§ª **Numerically stable**: careful `log1p`, `exp`, and `abs` usage to avoid overflow/underflow.  
- ğŸ§© **Drop-in replacement**: works as a substitute for `ReLU`, `Softplus`, or `SiLU` in most models.  
- ğŸ“Š **Gradient-friendly**: fully differentiable and backprop-compatible across frameworks.  
- ğŸ“¦ **MIT License**: free for academic and commercial use.  

### ğŸ”¬ Biologically Inspired Extensions
- ğŸ§  **Presets for neuron types**: built-in settings (`retinal_on`, `retinal_off`, `lgn_opponent`, `v1_simple`, `v1_complex`, `inhibitory`) inspired by retinal, LGN, and V1 cell responses.  
- ğŸ² **Jitter option**: introduces controlled random variation in parameters to simulate biological heterogeneity and diversify unit responses.  
- ğŸ”„ **Flexible parameterization**: smoothly interpolates between ReLU-like, Softplus-like, and Nakaâ€“Rushton-like behaviors by tuning Î±, Î², Î¸, p, Î´.  
- ğŸ“‰ **Derivative-friendly**: exposes both activation and derivative (gain curve) for analysis and visualization.  
- ğŸ–¼ï¸ **Visualization tools**: included plotting scripts to compare presets, derivatives, and jittered populations against standard activations.  
- ğŸ“– **Educational value**: doubles as a teaching tool to illustrate how activation shapes influence learning and how they map to biological neurons.  

---

### Biological Presets
    Softplus-Power provides **presets** that approximate real neural response types:

 - `generic` â€“ neutral baseline (default)  
 - `retinal_on` â€“ early threshold, moderate slope  
 - `retinal_off` â€“ mirrored ON response with offset  
 - `lgn_opponent` â€“ steeper contrast gain control  
 - `v1_simple` â€“ clear thresholding, steeper slope  
 - `v1_complex` â€“ more compressive/saturating  
 - `inhibitory` â€“ elevated threshold and offset (sparse response)  

---

## Jitter (Heterogeneity)

 - Real neurons are not identical. To mimic population diversity, Softplus-Power allows a jitter option that introduces small random variations in initialization:
 - Prevents identical unit responses.
 - Encourages diverse feature learning.
 - Mimics biological heterogeneity.
---

## ğŸ“¦ Installation

 ```bash
 git clone https://github.com/daa2bg/softplus-power.git
 cd softplus-power
 pip install -e .
 ```

##  Examples
**PyTorch examples:**

 ```python
 from softplus_power import CalibratedSoftplusPowerTorch
 act = CalibratedSoftplusPowerTorch(preset="v1_simple")
 ```
 ```python
 from softplus_power import CalibratedSoftplusPowerTorch
 act = CalibratedSoftplusPowerTorch(preset="v1_simple", jitter=0.05)
 ```
**Keras examples:**

  ```python
 from softplus_power import CalibratedSoftplusPowerKeras
 act = CalibratedSoftplusPowerKeras(preset="retinal_on")
 ```
 ```python
 from softplus_power import CalibratedSoftplusPowerKeras
 act = CalibratedSoftplusPowerKeras(preset="lgn_opponent", jitter=0.1)
 ```
---

###  Visualization
 A plotting script is included to visualize presets, derivatives, and jitter effects:

 python scripts/plot_activations.py --preset v1_simple --jitter 0.1 --save figs



### ğŸ”¹ Citation

If you use this activation in academic work, you can cite:
@misc{softpluspower2025,
  title   = {Softplus-Power: A Biologically Inspired Activation Function},
  author  = {Daniel Angelov},
  year    = {2025},
  url     = {https://github.com/daa2bg/softplus-power.git}
}
